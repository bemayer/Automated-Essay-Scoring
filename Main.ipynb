{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "flying-garden",
   "metadata": {},
   "source": [
    "# RCP 216 - Projet - Automated Essay Scoring"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recognized-caribbean",
   "metadata": {},
   "source": [
    "## Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "academic-joining",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import math\n",
    "import string\n",
    "import kaggle\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import language_check\n",
    "from os.path import exists\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import first, udf, split, flatten, col\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.stat import Summarizer\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler, VectorSlicer, Word2Vec\n",
    "\n",
    "from sparknlp.base import DocumentAssembler, Finisher, EmbeddingsFinisher, LightPipeline\n",
    "from sparknlp.annotator import Tokenizer, Normalizer, StopWordsCleaner, SentenceDetector, WordEmbeddingsModel, SentenceEmbeddings\n",
    "from sparknlp.pretrained import LemmatizerModel\n",
    "\n",
    "from sklearn.svm import LinearSVR\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "\n",
    "from keras.layers import LSTM, Dense, Dropout\n",
    "from keras.models import Sequential\n",
    "\n",
    "# Download data\n",
    "if not exists('Data'):\n",
    "    kaggle.api.authenticate()\n",
    "    kaggle.api.competition_download_files('asap-aes')\n",
    "    zipfile.ZipFile('asap-aes.zip').extractall(path='Data')\n",
    "\n",
    "\n",
    "# Create SparkSession\n",
    "sc = SparkSession.builder \\\n",
    "    .appName(\"Spark NLP\")\\\n",
    "    .master(\"local[4]\")\\\n",
    "    .config(\"spark.driver.memory\",\"16G\")\\\n",
    "    .config(\"spark.driver.maxResultSize\", \"0\") \\\n",
    "    .config(\"spark.kryoserializer.buffer.max\", \"2000M\")\\\n",
    "    .config(\"spark.jars.packages\", \"com.johnsnowlabs.nlp:spark-nlp_2.12:3.0.0\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Import data\n",
    "data = sc.read.csv('./Data/training_set_rel3.tsv', sep='\\t',\n",
    "    encoding='windows-1252', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "confirmed-colors",
   "metadata": {},
   "source": [
    "## Analyses exploratoires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "centered-thailand",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Show infos\n",
    "# data.printSchema()\n",
    "# data.createOrReplaceTempView('data')\n",
    "# data_rdd = data.rdd\n",
    "# data_pd = data.toPandas()\n",
    "# data_pd.info()\n",
    "\n",
    "# # Show essay count by essay subject\n",
    "# query = '''SELECT essay_set as Subject, COUNT(essay) as Count FROM data GROUP BY essay_set\n",
    "#     ORDER BY essay_set'''\n",
    "# essay_nb = sc.sql(query).toPandas()\n",
    "# fig, ax = plt.subplots()\n",
    "# ax.bar(essay_nb['Subject'], essay_nb['Count'])\n",
    "# plt.title('Essay count by Subject')\n",
    "# plt.xlabel('Subject')\n",
    "# plt.ylabel('Count')\n",
    "# plt.show()\n",
    "\n",
    "# # Show summary of scores by subject\n",
    "\n",
    "# query = '''SELECT essay_set as Subject, min(domain1_score) as Min,\n",
    "#     max(domain1_score) as Max, count(domain1_score) as Nb,\n",
    "#     count(distinct domain1_score) as Unique,\n",
    "#     format_number(avg(domain1_score), '#.##') as Avg,\n",
    "#     format_number(stddev(domain1_score), '#.##') as StDev\n",
    "#     FROM data GROUP BY essay_set ORDER BY Subject'''\n",
    "# sc.sql(query).show()\n",
    "\n",
    "# # Boxplot of scores by subject with bins\n",
    "\n",
    "# # Show distribution of word counts\n",
    "# data_pd.hist(column='word_count', by='topic', bins=25, sharey=True, sharex=True, layout=(2, 4), figsize=(7,4), rot=0)\n",
    "# plt.suptitle('Word count by topic #')\n",
    "# plt.xlabel('Number of words')\n",
    "# plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "departmental-gross",
   "metadata": {},
   "source": [
    "## Normalisation du score et échantillonage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "surprising-luther",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.filter('domain1_score is not null')\n",
    "\n",
    "scores_by_set = {}\n",
    "assembler = (VectorAssembler().setInputCols(['domain1_score'])\n",
    "    .setOutputCol('domain1_score_vector'))\n",
    "scaler = (StandardScaler().setWithMean(True)\n",
    "    .setInputCol('domain1_score_vector').setOutputCol('score_vector'))\n",
    "\n",
    "for set in range(1,9):\n",
    "    scores_by_set[set] = data.select('essay_id', 'essay_set',\n",
    "        'domain1_score').filter('essay_set == ' + str(set))\n",
    "    scores_by_set[set] = assembler.transform(scores_by_set[set])\n",
    "    scores_by_set[set] = (scaler.fit(scores_by_set[set])\n",
    "        .transform(scores_by_set[set]).toPandas().set_index('essay_id'))\n",
    "\n",
    "# Comment récupérer le premier élément d'un vecteur de manière vectorielle ?\n",
    "# https://stackoverflow.com/questions/38110038/spark-scala-how-to-convert-dataframevector-to-dataframef1double-fn-d\n",
    "for set in range(1,9):\n",
    "    score = [[essay, scores_by_set[set]['score_vector'][essay][0]] for essay in scores_by_set[set].index]\n",
    "    score = pd.DataFrame(score, columns=['essay_id', 'score']).set_index('essay_id')\n",
    "    scores_by_set[set] = pd.concat([scores_by_set[set], score], axis = 1)\n",
    "\n",
    "scores = pd.concat(scores_by_set.values())[['essay_set', 'score']]\n",
    "round(scores.score.std(), 3)\n",
    "round(scores.score.mean(), 3)\n",
    "\n",
    "rank_by_set = {}\n",
    "for set in range(1,9):\n",
    "    rank_by_set[set] = scores.loc[scores['essay_set'] == set]['score'].rank(pct=True, method ='first')\n",
    "    \n",
    "rank = pd.concat(rank_by_set.values())\n",
    "rank = rank.rename('rank')\n",
    "\n",
    "scores = pd.concat([scores, rank], axis=1)\n",
    "\n",
    "scores.loc[scores['rank'] <= 1/5, 'rank_group'] = '0'\n",
    "scores.loc[(scores['rank'] > 1/5) & (scores['rank'] <= 2/5), 'rank_group'] = '1'\n",
    "scores.loc[(scores['rank'] > 2/5) & (scores['rank'] <= 3/5), 'rank_group'] = '2'\n",
    "scores.loc[(scores['rank'] > 3/5) & (scores['rank'] <= 4/5), 'rank_group'] = '3'\n",
    "scores.loc[scores['rank'] > 4/5, 'rank_group'] = '4'\n",
    "\n",
    "table_count = pd.pivot_table(scores, values='score', index=['essay_set'],\n",
    "    columns=['rank_group'], aggfunc=pd.Series.count, margins = True)\n",
    "table_mean = pd.pivot_table(scores, values='score', index=['essay_set'],\n",
    "    columns=['rank_group'], aggfunc=pd.Series.mean, margins = True)\n",
    "\n",
    "scores_train = scores.sample(frac=0.7, random_state=42)\n",
    "scores_test = scores.loc[np.setdiff1d(scores.index, scores_train.index)]\n",
    "\n",
    "table_count = pd.pivot_table(scores_train, values='score', index=['essay_set'],\n",
    "    columns=['rank_group'], aggfunc=pd.Series.count, margins = True)\n",
    "table_mean = pd.pivot_table(scores_train, values='score', index=['essay_set'],\n",
    "    columns=['rank_group'], aggfunc=pd.Series.mean, margins = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indirect-playback",
   "metadata": {},
   "source": [
    "## Calcul de caractéristiques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "continued-dutch",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nb_words(str):\n",
    "    return(len(str.split()))\n",
    "\n",
    "def nb_organization(str):\n",
    "    return(str.count('@ORGANIZATION'))\n",
    "\n",
    "def nb_caps(str):\n",
    "    return(str.count('@CAPS'))\n",
    "\n",
    "def nb_person(str):\n",
    "    return(str.count('@PERSON') + str.count('@DR'))\n",
    "\n",
    "def nb_location(str):\n",
    "    return(str.count('@LOCATION') + str.count('@CITY') + str.count('@STATE'))\n",
    "\n",
    "def nb_money(str):\n",
    "    return(str.count('@MONEY'))\n",
    "\n",
    "def nb_time(str):\n",
    "    return(str.count('@TIME'))\n",
    "\n",
    "def nb_date(str):\n",
    "    return(str.count('@DATE') + str.count('@MONTH'))\n",
    "\n",
    "def nb_percent(str):\n",
    "    return(str.count('@PERCENT') + str.count('@NUM'))\n",
    "\n",
    "def Compute_Features(data):\n",
    "    nb_wordsUdf = udf(lambda str: nb_words(str), IntegerType())\n",
    "    nb_organizationUdf = udf(lambda str: nb_organization(str), IntegerType())\n",
    "    nb_capsUdf = udf(lambda str: nb_caps(str), IntegerType())\n",
    "    nb_personUdf = udf(lambda str: nb_person(str), IntegerType())\n",
    "    nb_locationUdf = udf(lambda str: nb_location(str), IntegerType())\n",
    "    nb_moneyUdf = udf(lambda str: nb_money(str), IntegerType())\n",
    "    nb_timeUdf = udf(lambda str: nb_time(str), IntegerType())\n",
    "    nb_dateUdf = udf(lambda str: nb_date(str), IntegerType())\n",
    "    nb_percentUdf = udf(lambda str: nb_percent(str), IntegerType())\n",
    "    data = data.withColumn('nb_words', nb_wordsUdf(data.essay))\n",
    "    data = data.withColumn('nb_organization', nb_organizationUdf(data.essay))\n",
    "    data = data.withColumn('nb_caps', nb_capsUdf(data.essay))\n",
    "    data = data.withColumn('nb_person', nb_personUdf(data.essay))\n",
    "    data = data.withColumn('nb_location', nb_locationUdf(data.essay))\n",
    "    data = data.withColumn('nb_money', nb_moneyUdf(data.essay))\n",
    "    data = data.withColumn('nb_time', nb_timeUdf(data.essay))\n",
    "    data = data.withColumn('nb_date', nb_dateUdf(data.essay))\n",
    "    data = data.withColumn('nb_percent', nb_percentUdf(data.essay))\n",
    "    return data\n",
    "\n",
    "data = Compute_Features(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "metropolitan-sandwich",
   "metadata": {},
   "source": [
    "## Correction grammaticale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "boring-china",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dear local newspaper, I think effects computers have on people are great learning skills/affects because they give us time to chat with friends/new people, helps us learn about the globe(astronomy) and keeps us out of troble! Thing about! Dont you think so? How would you feel if your teenager is always on the phone with friends! Do you ever time to chat with your friends or buisness partner about things. Well now - there's a new way to chat the computer, theirs plenty of sites on the internet to do so: @ORGANIZATION1, @ORGANIZATION2, @CAPS1, facebook, myspace ect. Just think now while your setting up meeting with your boss on the computer, your teenager is having fun on the phone not rushing to get off cause you want to use it. How did you learn about other countrys/states outside of yours? Well I have by computer/internet, it's a new way to learn about what going on in our time! You might think your child spends a lot of time on the computer, but ask them so question about the economy, sea floor spreading or even about the @DATE1's you'll be surprise at how much he/she knows. Believe it or not the computer is much interesting then in class all day reading out of books. If your child is home on your computer or at a local library, it's better than being out with friends being fresh, or being perpressured to doing something they know isnt right. You might not know where your child is, @CAPS2 forbidde in a hospital bed because of a drive-by. Rather than your child on the computer learning, chatting or just playing games, safe and sound in your home or community place. Now I hope you have reached a point to understand and agree with me, because computers can have great effects on you or child because it gives us time to chat with friends/new people, helps us learn about the globe and believe or not keeps us out of troble. Thank you for listening.\n"
     ]
    },
    {
     "ename": "Error",
     "evalue": "Exception in thread \"main\" java.lang.NoClassDefFoundError: javax/xml/bind/JAXBException\n\tat net.loomchild.segment.srx.io.Srx2SaxParser.<init>(Srx2SaxParser.java:173)\n\tat org.languagetool.tokenizers.SrxTools.createSrxDocument(SrxTools.java:51)\n\tat org.languagetool.tokenizers.SRXSentenceTokenizer.<init>(SRXSentenceTokenizer.java:53)\n\tat org.languagetool.tokenizers.SimpleSentenceTokenizer.<init>(SimpleSentenceTokenizer.java:37)\n\tat org.languagetool.Language.<clinit>(Language.java:60)\n\tat java.base/java.lang.Class.forName0(Native Method)\n\tat java.base/java.lang.Class.forName(Class.java:315)\n\tat org.languagetool.Languages.createLanguageObjects(Languages.java:111)\n\tat org.languagetool.Languages.getAllLanguages(Languages.java:97)\n\tat org.languagetool.Languages.<clinit>(Languages.java:39)\n\tat org.languagetool.language.LanguageIdentifier.getLanguageCodes(LanguageIdentifier.java:77)\n\tat org.languagetool.language.LanguageIdentifier.<init>(LanguageIdentifier.java:64)\n\tat org.languagetool.server.LanguageToolHttpHandler.<init>(LanguageToolHttpHandler.java:85)\n\tat org.languagetool.server.HTTPServer.<init>(HTTPServer.java:99)\n\tat org.languagetool.server.HTTPServer.main(HTTPServer.java:145)\nCaused by: java.lang.ClassNotFoundException: javax.xml.bind.JAXBException\n\tat java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:581)\n\tat java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(ClassLoaders.java:178)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:522)\n\t... 15 more",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mError\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-cbd31ee78514>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mfirstEssay\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlimit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'essay'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoPandas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfirstEssay\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mcheck\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlanguage_check\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLanguageTool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'en-US'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfirstEssay\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Nombre d'erreurs:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheck\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage_check\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLanguageTool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'en-US'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorrect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfirstEssay\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/language_check/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, language, motherTongue, remote_server)\u001b[0m\n\u001b[1;32m    194\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_remote_server_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_server_is_alive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_server_on_free_port\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlanguage\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/language_check/__init__.py\u001b[0m in \u001b[0;36m_start_server_on_free_port\u001b[0;34m(cls)\u001b[0m\n\u001b[1;32m    331\u001b[0m             \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_url\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'http://{}:{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_HOST\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_port\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 333\u001b[0;31m                 \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_local_server\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    334\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mServerError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/language_check/__init__.py\u001b[0m in \u001b[0;36m_start_local_server\u001b[0;34m(cls)\u001b[0m\n\u001b[1;32m    375\u001b[0m                 \u001b[0mmatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_PORT_RE\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    376\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmatch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 377\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    378\u001b[0m                 \u001b[0mport\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mport\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_port\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mError\u001b[0m: Exception in thread \"main\" java.lang.NoClassDefFoundError: javax/xml/bind/JAXBException\n\tat net.loomchild.segment.srx.io.Srx2SaxParser.<init>(Srx2SaxParser.java:173)\n\tat org.languagetool.tokenizers.SrxTools.createSrxDocument(SrxTools.java:51)\n\tat org.languagetool.tokenizers.SRXSentenceTokenizer.<init>(SRXSentenceTokenizer.java:53)\n\tat org.languagetool.tokenizers.SimpleSentenceTokenizer.<init>(SimpleSentenceTokenizer.java:37)\n\tat org.languagetool.Language.<clinit>(Language.java:60)\n\tat java.base/java.lang.Class.forName0(Native Method)\n\tat java.base/java.lang.Class.forName(Class.java:315)\n\tat org.languagetool.Languages.createLanguageObjects(Languages.java:111)\n\tat org.languagetool.Languages.getAllLanguages(Languages.java:97)\n\tat org.languagetool.Languages.<clinit>(Languages.java:39)\n\tat org.languagetool.language.LanguageIdentifier.getLanguageCodes(LanguageIdentifier.java:77)\n\tat org.languagetool.language.LanguageIdentifier.<init>(LanguageIdentifier.java:64)\n\tat org.languagetool.server.LanguageToolHttpHandler.<init>(LanguageToolHttpHandler.java:85)\n\tat org.languagetool.server.HTTPServer.<init>(HTTPServer.java:99)\n\tat org.languagetool.server.HTTPServer.main(HTTPServer.java:145)\nCaused by: java.lang.ClassNotFoundException: javax.xml.bind.JAXBException\n\tat java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:581)\n\tat java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(ClassLoaders.java:178)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:522)\n\t... 15 more"
     ]
    }
   ],
   "source": [
    "firstEssay =  data.limit(1).select('essay').toPandas().squeeze()\n",
    "print(firstEssay)\n",
    "check = language_check.LanguageTool('en-US').check(firstEssay)\n",
    "print(\"Nombre d'erreurs:\", len(check))\n",
    "print(language_check.LanguageTool('en-US').correct(firstEssay, check))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "durable-appearance",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nb_error(str):\n",
    "    matches = language_check.LanguageTool('en-US').check(str)\n",
    "    nb_error = (len(matches))\n",
    "    return(nb_error)\n",
    "\n",
    "def correcter(str):\n",
    "    corrected = language_check.LanguageTool('en-US').correct(str)\n",
    "    return(corrected)\n",
    "\n",
    "# Remplacement par des espaces de charactères qui gènent la tokenisation de Spark\n",
    "def replace_char(str):\n",
    "    for char in ['\\\\', '/', '(', ')', '[', ']', '{', '}']:\n",
    "        str = str.replace(char, ' ')\n",
    "    return (str)\n",
    "\n",
    "def replace_anom(str):\n",
    "    for char in ['@ORGANIZATION.', '@CAPS.', '@PERSON.', '@LOCATION.', '@MONEY.', '@TIME.', '@DATE.', '@PERCENT.', '@MONTH.', '@NUM.', '@DR.', '@CITY.', '@STATE.']:\n",
    "        str = re.sub(char, ' ', str)\n",
    "    return (str)\n",
    "\n",
    "def Correct_Essay(data):\n",
    "    nb_errorUdf = udf(lambda str: nb_error(str), IntegerType())\n",
    "    correcterUdf = udf(lambda str: correcter(str), StringType())\n",
    "    replace_charUdf = udf(lambda str: replace_char(str), StringType())\n",
    "    replace_anomUdf = udf(lambda str: replace_anom(str), StringType())\n",
    "    data = data.withColumn('nb_orth_error', nb_errorUdf(data.essay))\n",
    "    data = data.withColumn('essay', correcterUdf(data.essay))\n",
    "    data = data.withColumn('essay', replace_charUdf(data.essay))\n",
    "    data = data.withColumn('essay', replace_anomUdf(data.essay))\n",
    "    return data\n",
    "\n",
    "data = Correct_Essay(data)\n",
    "    \n",
    "if not exists('Data/data_corrected.parquet'):\n",
    "    data.write.parquet('Data/data_corrected.parquet')\n",
    "\n",
    "data = sc.read.parquet('Data/data_corrected.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mounted-infection",
   "metadata": {},
   "source": [
    "## Pipeline de prétraitements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tender-representation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lemma_antbnc download started this may take some time.\n",
      "Approximate size to download 907.6 KB\n",
      "[OK!]\n",
      "glove_6B_300 download started this may take some time.\n",
      "Approximate size to download 426.2 MB\n",
      "[OK!]\n"
     ]
    }
   ],
   "source": [
    "documenter = (DocumentAssembler().setCleanupMode('shrink').setInputCol('essay')\n",
    "                .setOutputCol('document'))\n",
    "tokenizer = Tokenizer().setInputCols(['document']).setOutputCol('tokenized')\n",
    "normalizer = (Normalizer().setLowercase(True).setInputCols(['tokenized'])\n",
    "                .setOutputCol('normalized'))\n",
    "cleaner = (StopWordsCleaner().setInputCols(['normalized'])\n",
    "                .setOutputCol('cleaned'))\n",
    "lemmatizer = (LemmatizerModel.pretrained(name = 'lemma_antbnc', lang='en')\n",
    "                .setInputCols(['cleaned']).setOutputCol('lemmatized'))\n",
    "finisher = Finisher().setInputCols(['lemmatized']).setOutputCols('finished')\n",
    "vectorizer = (Word2Vec().setSeed(42).setVectorSize(300)\n",
    "                .setInputCol('finished').setOutputCol('vectorized'))\n",
    "vectorizer2 = (WordEmbeddingsModel.pretrained('glove_6B_300', 'xx')\n",
    "                .setInputCols('document', 'lemmatized')\n",
    "                .setOutputCol('embedded'))\n",
    "averager = SentenceEmbeddings().setPoolingStrategy(\"SUM\").setInputCols(['document', 'embedded']).setOutputCol('averaged')\n",
    "finisher2 = EmbeddingsFinisher().setInputCols(['averaged']).setOutputCols('vectorized')\n",
    "\n",
    "pipeline_w2v = Pipeline().setStages([documenter, tokenizer, normalizer, cleaner,\n",
    "                lemmatizer, finisher, vectorizer]).fit(data)\n",
    "pipeline_glove = Pipeline().setStages([documenter, tokenizer, normalizer, cleaner,\n",
    "                lemmatizer, vectorizer2, averager, finisher2]).fit(data)\n",
    "\n",
    "# Les Light Pipelines sont plus rapides ?\n",
    "# https://medium.com/spark-nlp/spark-nlp-101-lightpipeline-a544e93f20f1\n",
    "pipeline_w2v_light = LightPipeline(pipeline_w2v)\n",
    "pipeline_glove_light = LightPipeline(pipeline_glove)\n",
    "\n",
    "\n",
    "if not exists('Data/data_w2v.parquet'):\n",
    "    data_w2v = pipeline_w2v_light.transform(data)\n",
    "    data_w2v = data_w2v.drop('finished')\n",
    "    data_w2v.write.parquet('Data/data_w2v.parquet')\n",
    "\n",
    "data_w2v_pd = pd.read_parquet('Data/data_w2v.parquet')\n",
    "\n",
    "if not exists('Data/data_glove.parquet'):\n",
    "    data_glove = pipeline_glove_light.transform(data)\n",
    "    data_glove = data_glove.drop('document', 'tokenized', 'normalized',\n",
    "        'cleaned', 'lemmatized', 'embedded', 'averaged')\n",
    "    data_glove.write.parquet('Data/data_glove.parquet')\n",
    "\n",
    "data_glove_pd = pd.read_parquet('Data/data_glove.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accomplished-estimate",
   "metadata": {},
   "source": [
    "## Création et enregistrement des données d'entraînement et de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "about-ukraine",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_w2v_pd = data_w2v_pd.set_index('essay_id')\n",
    "data_glove_pd = data_glove_pd.set_index('essay_id')\n",
    "\n",
    "selected = ['essay_set'] + [s for s in data.columns if 'nb' in s]\n",
    "vec_names = ['vec_' + str(i) for i in range(0, 300)]\n",
    "\n",
    "vector_w2v = [[essay, *(data_w2v_pd['vectorized'][essay]['values'])] for essay in data_w2v_pd['vectorized'].index]\n",
    "vector_w2v = pd.DataFrame(vector_w2v, columns=['essay_id', *vec_names]).set_index('essay_id')\n",
    "\n",
    "vector_glove = [[essay, *(data_glove_pd['vectorized'][essay][0])] for essay in data_glove_pd['vectorized'].index]\n",
    "vector_glove = pd.DataFrame(vector_glove, columns=['essay_id', *vec_names]).set_index('essay_id')\n",
    "\n",
    "X_w2v = pd.concat([data_w2v_pd[selected], vector_w2v], axis = 1)\n",
    "X_glove = pd.concat([data_glove_pd[selected], vector_glove], axis = 1)\n",
    "\n",
    "X_w2v_train = X_w2v.loc[scores_train.index]\n",
    "X_w2v_test = X_w2v.loc[scores_test.index]\n",
    "\n",
    "X_glove_train = X_glove.loc[scores_train.index]\n",
    "X_glove_test = X_glove.loc[scores_test.index]\n",
    "\n",
    "if not exists('Data/X_w2v_train.taz'):\n",
    "    X_w2v_train.to_csv('Data/X_w2v_train.taz', compression='gzip')\n",
    "if not exists('Data/X_w2v_test.taz'):\n",
    "    X_w2v_test.to_csv('Data/X_w2v_test.taz', compression='gzip')\n",
    "if not exists('Data/X_glove_train.taz'):\n",
    "    X_glove_train.to_csv('Data/X_glove_train.taz', compression='gzip')\n",
    "if not exists('Data/X_glove_test.taz'):\n",
    "    X_glove_test.to_csv('Data/X_glove_test.taz', compression='gzip')\n",
    "if not exists('Data/y_train.taz'):\n",
    "    scores_train.to_csv('Data/y_train.taz', compression='gzip')\n",
    "if not exists('Data/y_test.taz'):\n",
    "    scores_test.to_csv('Data/y_test.taz', compression='gzip')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "standing-shirt",
   "metadata": {},
   "source": [
    "## Modélisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "finnish-tulsa",
   "metadata": {},
   "source": [
    "### Chargement des données pour modélisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "little-sacrifice",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Score(model, train_error, test_error):\n",
    "    return pd.Series((model, train_error, test_error), index = ('Model', 'Train_error', 'Test_error'), name = model)\n",
    "\n",
    "def Load_X(vec):\n",
    "    names = {sample: 'X_' + vec + '_' + sample for sample in ['train', 'test']}\n",
    "    data = {}\n",
    "    for key, obj in names.items():\n",
    "        if obj in globals():\n",
    "            data[key] = globals()[obj]\n",
    "        else:\n",
    "            data[key] = train = pd.read_csv('Data/' + obj + '.taz', compression='gzip').set_index('essay_id')\n",
    "    return(data)\n",
    "\n",
    "def Load_y():\n",
    "    names = {sample: 'y_' + sample for sample in ['train', 'test']}\n",
    "    data = {}\n",
    "    for key, obj in names.items():\n",
    "        if obj in globals():\n",
    "            data[key] = globals()[obj]['score']\n",
    "        else:\n",
    "            data[key] = pd.read_csv('Data/' + obj + '.taz', compression='gzip').set_index('essay_id')['score']\n",
    "    return(data)\n",
    "\n",
    "if not 'score_log' in globals():\n",
    "    if not exists('Data/score_log.csv'):\n",
    "        score_log = pd.DataFrame(columns=('Model', 'Train_error', 'Test_error'))\n",
    "    else:\n",
    "        score_log = pd.read_csv('Data/score_log.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "boring-pendant",
   "metadata": {},
   "source": [
    "### Régression Linéaire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "running-helping",
   "metadata": {},
   "outputs": [],
   "source": [
    "for vec in ['w2v', 'glove']:\n",
    "    X = Load_X(vec)\n",
    "    y = Load_y()\n",
    "    model_name = 'LR_' + vec\n",
    "    if model_name not in score_log['Model'].values:\n",
    "        model = LinearRegression().fit(X['train'], y['train'])\n",
    "        y_pred_train = pd.Series(model.predict(X['train']), index=X['train'].index)\n",
    "        y_pred_test = pd.Series(model.predict(X['test']), index=X['test'].index)\n",
    "        mse_train = mean_squared_error(y['train'], y_pred_train)\n",
    "        mse_test = mean_squared_error(y['test'], y_pred_test)\n",
    "        score_log = score_log.append(Score(model_name, mse_train, mse_test))\n",
    "\n",
    "score_log.to_csv('Data/score_log.csv', index=False)"
   ]
  },
  {
   "source": [
    "### Régression Linéaire par Sujet"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for vec in ['w2v', 'glove']:\n",
    "    for set in range(1, 9):\n",
    "        X = Load_X('w2v')\n",
    "        X = {sample: X[sample][X[sample]['essay_set'] == set] for sample in ['test', 'train']}\n",
    "        y = Load_y()\n",
    "        y = {sample: y[sample][X[sample].index] for sample in ['test', 'train']}\n",
    "        model_name = 'LR_' + vec + '_' + str(set)\n",
    "        if model_name not in score_log['Model'].values:\n",
    "            model = LinearRegression().fit(X['train'], y['train'])\n",
    "            y_pred_train = pd.Series(model.predict(X['train']), index=X['train'].index)\n",
    "            y_pred_test = pd.Series(model.predict(X['test']), index=X['test'].index)\n",
    "            mse_train = mean_squared_error(y['train'], y_pred_train)\n",
    "            mse_test = mean_squared_error(y['test'], y_pred_test)\n",
    "            score_log = score_log.append(Score(model_name, mse_train, mse_test))\n",
    "\n",
    "score_log.to_csv('Data/score_log.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "effective-leone",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "found-offer",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SVM(X, y, C):\n",
    "    model = LinearSVR(C=C, max_iter=100000)\n",
    "    model.fit(X['train'], y['train'])\n",
    "    y_pred_train = pd.Series(model.predict(X['train']), index=X['train'].index)\n",
    "    y_pred_test = pd.Series(model.predict(X['test']), index=X['test'].index)\n",
    "    mse_train = mean_squared_error(y['train'], y_pred_train)\n",
    "    mse_test = mean_squared_error(y['test'], y_pred_test)\n",
    "    return mse_train, mse_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "complex-prize",
   "metadata": {},
   "outputs": [],
   "source": [
    "for vec in ['w2v', 'glove']:\n",
    "    X = Load_X(vec)\n",
    "    y = Load_y()\n",
    "    for C in [ 10 ** x for x in range(-5, 5)]:\n",
    "        model_name = 'SVM_' + vec + '_' + str(C)\n",
    "        if model_name not in score_log['Model'].values:\n",
    "            score_log = score_log.append(Score(model_name, *(SVM(X, y, C))))\n",
    "\n",
    "score_log.to_csv('Data/score_log.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "political-diagnosis",
   "metadata": {},
   "source": [
    "### LTSM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pacific-infrared",
   "metadata": {},
   "outputs": [],
   "source": [
    "def NN_2(layer_1, layer_2):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(layer_1))\n",
    "    model.add(Dense(layer_2))\n",
    "    model.add(Dense(1, activation='linear'))\n",
    "    model.compile(loss='mean_squared_error', optimizer='rmsprop', metrics=['mae'])\n",
    "    return model\n",
    "    \n",
    "def train_NN(model_name, X, y, layer_1, layer_2):\n",
    "    model = NN_2(layer_1, layer_2)\n",
    "    history = model.fit(X['train'], y['train'], epochs=50,\n",
    "                        batch_size=128, validation_data=(X['test'], y['test']))\n",
    "    epo = np.array(history.epoch)\n",
    "    acc_train = np.array(history.history['mae'])\n",
    "    acc_test = np.array(history.history['val_mae'])\n",
    "    log = np.c_[epo, acc_train, acc_test]\n",
    "    np.savetxt('./Data/' + model_name + '.csv', log, delimiter=',')\n",
    "    loss_train, mse_train = model.evaluate(X['test'], y['test'])\n",
    "    loss_test, mse_test = model.evaluate(X['train'], y['train'])\n",
    "    return mse_train, mse_test\n",
    "\n",
    "for vec in ['w2v', 'glove']:\n",
    "    X = Load_X(vec)\n",
    "    y = Load_y()\n",
    "    for layer_1 in [10, 25, 50, 100, 200, 300]:\n",
    "        for layer_2 in [10, 25, 50, 100, 200, 300]:\n",
    "            model_name = 'NN_' + vec + '_' + str(layer_1) + '_' + str(layer_2)\n",
    "            if model_name not in score_log['Model'].values:\n",
    "                score_log = score_log.append(Score(model_name, *(train_NN(model_name, X, y, layer_1, layer_2))))"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-4.m65",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-4:m65"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}